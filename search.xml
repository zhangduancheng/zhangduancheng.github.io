<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[毕业了]]></title>
    <url>%2F2019%2F06%2F25%2F%E6%AF%95%E4%B8%9A%E4%BA%86%2F</url>
    <content type="text"><![CDATA[毕业合影只有这一个视频，150块钱完完全全被人给坑了，关键是这剪的水平也太次了，虽说我全程掉线吧！]]></content>
      <tags>
        <tag>生活</tag>
        <tag>毕业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元分类问题-2]]></title>
    <url>%2F2019%2F06%2F19%2F%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98-2%2F</url>
    <content type="text"><![CDATA[上一篇文章（多元分类问题-1）说到了多元分类问题的直接解决方案，这里讨论 将多元分类问题降维到二元分类问题上。将多元分类问题分解成多个二元分类问题常用的 方法有两种，一种是OvR/OvA（one-vs-rest），另一种是OvO（One-vs-One）。 OvR/OvA假设这里有一个三元分类问题，$Y_i$可能的取值为0,1,2,对于这样的一个三元分类问题，可以将其缩小成为一个小的二元分类问题，我们将其定义为$Y_i=0$或$Y_i≠0$，我们可以认为1事件为$Y_i=0$,0事件为$Y_i≠0$，对于这样一个二元分类问题，我们可以用逻辑回归来解决，相应的我们也得到了$Y_i=0$的概率，同样我们也可以将其分解成$Y_i=1(Y_i≠1)$或$Y_i=2(Y_i≠2)$，相应的我们可以得到$P(Y_i=1)$或$P(Y_i=2)$。事实上，我们得到的概率并不是真正的概率，也即$P(Y_i=0)+P(Y_i=1)+P(Y_i=2)$不一定等于1，这是因为这个概率只是表示$Y_i$等于某个值的可能性有多大 ，有了这些虚假的、假定的概率后，我们从中选择概率值最大的，作为最终的预测结果。假设$P(Y_i=0)&gt;P(Y_i=1)&gt;P(Y_i=2)$,那么我就认为$Y_i$的预测值即Y^=0，通过这种OvR的分解，我们就把这个三元分类问题分解成三个二元分类问题来解决。 OvO与OvR相对应的是OvO，我们这样来理解OvO，对于同样的三元分类问题，我们将$Y_i=0$与$Y_i=1$单拎出来,这样数据里面只有两种可能的取值，要么等于0，要么等于1，对于这样的二元分类问题，可以使用逻辑回归解决；同样的，我们将$Y_i=1$与$Y_i=2$单拎出来，就得到另外的一个二元元分类问题，对于这样的二元分类问题，同样可以使用逻辑回归来解决；类似的可以得到第三个二元分类问题$Y_i=0$与$Y_i=2$。需要注意的是，在OvO这个方法中，对于每一个子问题，假设这个子问题是$Y_i=0$与$Y_i=1$,它使用的数据是$Y_i=0$与$Y_i=1$的数据，也就是说它把$Y_i$可能的取值单独拎出来，让它们之间互相PK；但是对于OvR来讲，它使用的数据是$Y_i=0$与$Y_i≠0$，也就是说$Y_i=0$它所PK的是除了它之外的剩下的数据，故它的名字叫做OvR(One vs rest)——一个对抗剩下的所有，而OvO是一个对抗一个。对于这样一个三个二元分类问题，我们可以依次的解决，而且需要注意到，$Y_i=0$它参加了两场比赛 ，$Y_i=1$也参加了两场比赛，$Y_i=2$也参加连两场比赛，而每一场比赛都能选择出来一个胜利者，而获胜次数最多的，将作为最终的预测结果。 总结1234对于多元分类问题，我们可以使用多种方式去结果：1.直接使用多元逻辑回归，这里我们利用的是K个隐含变量模型；2.OvA是将一个多元分类问题分解成了K个二元分类子问题;3.OvO是将一个多元分类问题分解成k(k-1)/2个二元分类子问题。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元分类问题-1]]></title>
    <url>%2F2019%2F06%2F12%2F%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98-1%2F</url>
    <content type="text"><![CDATA[对于多元分类问题而言，直接使用多元逻辑回归就可以直接解决多元分类问题，同时这里的多元逻辑回归和逻辑回归不同，一般的逻辑回归只能解决二元分类问题。 多元逻辑回归的实现可以参考二元分类问题解决的思想，即从隐含变量出发，推导出各个类别的概率。 首先定义k个隐含变量模型，分别是$Y_ {i,0}^*$,$Y_ {i,1}^*$,…,$Y_ {i,k-1}^*$,其中$Y_ {m,1}^*$表示类别l对类别m的效用，这仅仅是对二元类别的扩展。同样，与二元类别类似，假设类别1对个体m的效用为公式(2-1)所表示的线性模型，其线性部分即为$X_i\theta$,$\epsilon$为随机扰动项。在讨论逻辑回归模型的时候，假设$\epsilon$是服从逻辑分布的，可以近似地认为是服从正态分布的，因为逻辑分布和正态分布的差别比较小；但在多元逻辑分布的情况下，随机扰动项 服从标准的类型1的极端值分布（与正态分布的形状几乎一致，尾巴比正态分布厚一点），做出这种假设的目的是为了推导出比较好运算的公式，在二元分类问题上因为正态分布无法推导出比较优雅的数学公式，只能得出Probit回归，这在数学上很难处理，所以需要使用逻辑回归，这在多元逻辑回归上是同样的道理。 有了k个隐含变量模型就可以定义第i个个体属于某个类的逻辑（个体属于某个类别，当且仅当这个类别对它的效用最大），也就是说，当假设$Y_{i,0}^*$是所有$Y_{i,j}^*$中最大值时，就认为i这个个体属于0这个类别，也就是$Y_i=0$，以此类推，可以得到公式2-2所示的$Y_i$等于各个类别的概率。通过公式2-2，经过复杂的数学运算，可以推导出各个类别的概率分布，如公式2-3所示：从中可以看到$Y_i=0$的概率分布公式和逻辑回归的概率分布回归公式很类似（若k=2，此概率分布公式即为正常逻辑回归情况下的概率分布公式），通过iY=0可以一直推导出$Y_i=k-1$时的概率分布公式。在得到各类别的概率分布公式后，接下来要做的是求出整个模型似然函数，也就是真实值的分布概率值，即公式2-4：但是这个概率值是一个乘积的关系，为了方便计算，在公式两边同时取Ln，如公式2-5所示，这样乘积的关系就转换成和的关系，计算也更加方便。在得到模型的似然函数之后，模型参数的估计公式也就可以通过最大似然估计的方法得出了，即公式2-6：在得到模型参数的估计公式后，可以通过导入训练数据得到模型参数的估计值。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我们一起来看复联4啦]]></title>
    <url>%2F2019%2F04%2F24%2F%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%9D%A5%E7%9C%8B%E5%A4%8D%E8%81%944%E5%95%A6%2F</url>
    <content type="text"><![CDATA[自从复联3上映后，相信很多复联的粉丝就在期待着复联四的上映，在复联三结尾处由于雷神的失误，没有砍掉灭霸的头，导致灭霸一个响指，消灭了宇宙一半的人口，影片结束后，我们从各种渠道想要知悉复联四的动向，每一次预告片的发布，总能引起一阵轰动，推测剧中人物走向，毕竟是第三阶段的结束，自然会有旧英雄退场，新英雄接棒，不过旧英雄以何种方式退场便是我们关注的焦点。 4月23日，早早的起床收拾东西，不仅是要去看复联4，更重要的是要去见女朋友了，好开心，要和她一起去看首映咯！我之前是不看超级英雄的电影的，之所以成为漫威的粉丝，自然就是她的安利了！幸亏她告诉我，不然就错过了这么伟大的一系列电影！首映场，人自然很多，之前放票的时候就紧盯着售票软件，不停的刷新，果真让我抢到了，当时真是高兴了一个下午啊！之后半个月都在买到票的窃喜中！坐了一个多小时的高铁🚄，来到了女友所在的城市，将东西放下后，准备收拾出门去！电影院里，虽然赶不上春运，但是也可以说得上是人山人海，幸亏我们去的还算早，在等候的时候还有座位可做，提前到了一个多小时。 ·················· 三个小时结束了，真的是在首映场才能感受到的感觉，到了高兴处大家可以一起鼓掌，因为大家可以一起鼓掌、高呼，没人会不开心，因为大家都明白！这样的结局应该是最好的安排了吧，虽然女友哭的稀里糊涂的，整场电影院里满是泣声，一切都落下帷幕了！最后补一张我铁的英雄照]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[亲爱的生日快乐]]></title>
    <url>%2F2019%2F03%2F14%2F%E4%BA%B2%E7%88%B1%E7%9A%84%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
